---
title: "tidytuesday_4"
author: "Gustavo"
date: "2022-12-27"
output: 
  html_document:
    toc: true
    theme: journal
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', message = FALSE, warning = FALSE)
```

# Board Games

This week #TidyTuesday *dataset* is about board games! I do some board gaming with some friends and we usually play some Catan, Carcassonne, Stone Age, Harbour, and other games. I'm really excited to do some analysis in this dataset and I hope I see some of the games I like to play and even acknowledge some other ones that I didn't know about.

Okay, so first we need to load the data and join the two pieces of data into only one data frame.

```{r}
# We load the tidyverse library
library(tidyverse)

# We load both the datasets, with the ratings and details
ratings <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv")
details <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv")

# Merging the two datasets by "id"
ratings_joined <- 
  ratings %>% 
  left_join(details, by = "id")
```

There are a lot of columns such as the 'url' that contains only the url with the logo of the game, and we are mainly not interested in it. Usually we'd just select those before we even begin to explore data. That's a good practice when dealing with median/large datasets.

# Exploratory Data Analysis (EDA)

One of the main ideas is to predict ratings based on other variables, or even understand what variables are related to the final rate of the game.

We can begin very simple, just plotting the histogram of the ratings:

```{r}
ratings_joined %>% 
  ggplot(aes(average))+
  geom_histogram(bins = 40, color = "brown", fill = "beige")+
  labs(x = "Average rating", y = "", title = "Game rating distribution")+
  theme_minimal()+
  theme(text = element_text(size = 14),
        title = element_text(size = 16, face = "bold"),
        plot.title =  element_text(size = 25, face = "bold", hjust = 0.5))
```

We can see that the average rating of the games in this dataset follows closely a normal distribution and it is expected. Now let's see what some variables such as 'minage' or 'maxplayers' is related to the ratings prior of build any model. Drawing some plots:

```{r fig.width= 8, fig.height= 6}
ratings_joined %>% 
  filter(!is.na(minage)) %>% 
  mutate(minage = cut_number(minage, 4)) %>%
  ggplot(aes(x = minage, y = average, color = minage, fill = minage))+
  geom_boxplot(show.legend = FALSE, alpha = 0.6)+
  theme_minimal()+
  labs(x = "Minimum age for playing the game",
       y = "Average ratings")+
  theme(text = element_text(size = 13),
        title = element_text(size = 14, face = "bold"))
  
```

Now for the 'playing time' variable:

```{r fig.width= 8, fig.height= 6}
ratings_joined %>% 
  filter(!is.na(playingtime)) %>% 
  mutate(playingtime = cut_number(playingtime, 4)) %>%
  ggplot(aes(x = playingtime, y = average, color = playingtime, fill = playingtime))+
  geom_boxplot(show.legend = FALSE, alpha = 0.6)+
  theme_minimal()+
  labs(x = "Playing time",
       y = "Average ratings")+
  theme(text = element_text(size = 13),
        title = element_text(size = 14, face = "bold"))
  
```

We can see the exactly same trend: as the minumum age and playing time increases the average ratings for that specific game increases rather slightly.

We can proceed to build the models.

# Prep the data

I decided to try and apply some simple linear regression models so we can compare aftwards with the XGBoost models.

But first, we need to pre-process the data. I'm applying the same concepts of Julia Silge's on her analysis. Our 'data-budget' is only the name, average, the variables maching "min\|max" regex and boardgamecategory which is going to give us a lot of work to feature engineer it.

```{r}
library(tidymodels)

set.seed(123)

#Splitting the training and test datasets
game_split <-
  ratings_joined %>% 
  select(name, average, matches("min|max"), boardgamecategory) %>% 
  na.omit() %>% 
  initial_split(strata = average)
game_train <- training(game_split)
game_test <- testing(game_split)


# Doing the k-fold division with k = 10
set.seed(234)
game_folds <- vfold_cv(game_train, v = 10, strata = average)
```

Now she does this feature engineering using `textrecipes` and a customized function `split_category()`. After that ou work is done with pre-processing the *dataset*.

```{r}
library(textrecipes)

# Customized function to split the strings, remove all punctuation
## squish (trim), string to lower and replace all blank spaces with _
split_category <- function(x){
  x %>% 
    str_split(", ") %>% 
    map(str_remove_all, "[:punct:]") %>%
    map(str_squish) %>% 
    map(str_to_lower) %>% 
    map(str_replace_all, " ", "_")
  
}

games_rec <- recipe(average ~., data = game_train) %>% 
  update_role(name, new_role = "id") %>% 
  step_tokenize(boardgamecategory, custom_token = split_category) %>% 
  step_tokenfilter(boardgamecategory, max_tokens = 30) %>% 
  step_tf(boardgamecategory)

# Just to check
game_prep <- prep(games_rec)
bake(game_prep, new_data = NULL) %>%  str()
```

# Multiple Linear Regression Models

Specifying the linear regression:

```{r fig.height= 8, fig.align='center', fig.width=10}

# The recipe
reg_recipe <- recipe(average~., data = game_train) %>% 
  update_role(name, new_role = "id") %>% 
  step_tokenize(boardgamecategory, custom_token = split_category) %>% 
  step_tokenfilter(boardgamecategory, max_tokens = 30) %>% 
  step_tf(boardgamecategory)

# Specifying the linear regression
reg_spec <- linear_reg(mode = "regression", engine = "lm") 

# Doing the Workflow
reg_wf <- workflow() %>% 
  add_model(reg_spec)%>% 
  add_recipe(reg_recipe) 

# Setting the metrics
set.seed(123)
linear_metrics <- metric_set(rmse, rsq, ccc)

#Doing the resampling method to assess the metrics of Q²
reg_resamples <- reg_wf %>% 
  fit_resamples(game_folds, metrics = linear_metrics)

# Printing the CV Mean of RSQ and RMSE
reg_resamples %>% collect_metrics()
```

Seeing the plot below one can clearly see that a multiple linear regression is not enough to predict the testing set, that is, our MLR is not very good at predicting external data.

The purpose of the chunk below is to fit a linear regression using the workflow built above and do a final fit into the data by automatically selecting the best model using cross-validation. The R², RMSE and CCC for the CV-model (a mean of each of the respective metric) can be seen in the output above and now we're going to compare with the test set.

```{r}
# Doing the fit into the training data
reg_fit <- reg_wf %>% 
    last_fit(split = game_split, metrics = linear_metrics)

# Collecting the metrics
collect_metrics(reg_fit)

# Collecting the test prediction
reg_test_predictions <- collect_predictions(reg_fit)

reg_test_predictions %>% 
  ggplot(aes(.pred, average))+
  geom_point(size = 2, alpha = 0.2)+
  geom_abline(intercept = 0, slope = 1, lty = 5, colour = "gray")+
  coord_obs_pred()+
  theme_minimal()+
  theme(text = element_text(size = 13),
        title = element_text(size = 14, face = "bold"))+
  labs(title = "Linear Regression Prediction of Testing set", x = "Predicted", y = "Real")
```

Comparing the CV metrics (R²/also known as Q², CCC and RMSE) and the testing set metrics one can see that they're pretty similar, hence the power and importance of estimating the cross-validation metrics.

We can also see the training *dataset* predictions:

```{r}
#Extract the final model
final_reg_model <- extract_workflow(reg_fit)

# Do predictions for the training set using the final model
training_pred <- augment(final_reg_model, new_data = game_train)

# Plot actual vs predicted
training_pred %>% 
  ggplot(aes(.pred, average))+
  geom_point(size = 2, alpha = 0.2)+
  geom_abline(intercept = 0, slope = 1, lty = 5, colour = "gray")+
  coord_obs_pred()+
  theme_minimal()+
  theme(text = element_text(size = 13),
        title = element_text(size = 14, face = "bold"))+
  labs(title = "Linear Regression Prediction of **Training** set", x = "Predicted", y = "Real")

```

We can analyse the coefficients of the final model, if one can consider this a model.

```{r}
# Highest terms in the final linear equation
tidy(final_reg_model) %>%
  slice_max(abs(estimate), n = 10) %>% 
  mutate(term = fct_reorder(term, abs(estimate)))
```

# XGBoost Models

Now we finally go back to our xgboost models. If we're up to it, we could also do a RandomForest Model aswell.

```{r}
xgb_spec <- boost_tree(
  trees = tune(),
  mtry = tune(),
  min_n = tune(),
  learn_rate = 0.1
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgb_wf <- workflow(spec = xgb_spec, preprocessor = games_rec)
xgb_wf
```

Now we can generate the XGBoost models, using `finetune` library and `tune_race_anova()` function, so we don't waste too much time searching a grid with hyperparameters values that don't make sense to our scenario.

```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(234)
xgb_game_rs <-
  tune_race_anova(
    xgb_wf,
    game_folds,
    grid = 20,
    control = control_race(verbose_elim = T)
  )

xgb_game_rs

```

We can see the best model performed in the tuning/cross-validation process above:

```{r}
show_best(xgb_game_rs)
```

We then finalize the workflow and analyze the variable importance using `vip()` function of the vip library.

```{r}
last_xgb <- xgb_wf %>%
  finalize_workflow(select_best(xgb_game_rs, "rmse")) %>% 
  last_fit(game_split)

collect_metrics(last_xgb)
```

We can see the R² and RMSE of the final testing set. The RMSE is very close to the RMSE obtained in the CV process.

Plotting the testing set obs vs predicted plot so we can visually compare with the Multiple Linear Regression done above.
```{r}
collect_predictions(last_xgb) %>% 
  ggplot(aes(.pred, average))+
  geom_point(size =2, alpha =.2)+
  geom_abline(intercept = 0, slope = 1, lty = 5, colour = "gray")+
  coord_obs_pred()+
  theme_minimal()+
  theme(text = element_text(size=13),
        title= element_text(size = 14, face = "bold"))+
  labs(title = "XGBoost Model Prediciton of Testing Set",
       x = "Predicted", y = "Real")
```
Comparing with the MLP model done before we can clearly see a great improvement. Still, there is room for more improvement but that is fine.

Furthermore, analyzing the variable importance for our final model:
```{r fig.align='center', fig.height= 6, fig.width= 8}
library(vip)
# We have to extract the fit using the èxtract_fit_parsnip()` function to our last_fit object so it can work properly.
xgb_fit <- extract_fit_parsnip(last_xgb)
#Plotting the variable importance
vip(xgb_fit, geom = "point", num_features = 14)
```

Finally, we can make use of Shapley Additive Explanations with the `SHAPforxgboost` package, where average contribution of features are computed under different combinations.
```{r}
library(SHAPforxgboost)

game_shap <-
  shap.prep(
    xgb_model = extract_fit_engine(xgb_fit),
    X_train = bake(game_prep,
                   has_role("predictor"),
                   new_data = NULL, composition = "matrix"
                   ))
```
Plotting the partial contribution of each variable:
```{r fig.align='center',fig.height= 9, fig.width= 10}
shap.plot.summary(game_shap)
```











